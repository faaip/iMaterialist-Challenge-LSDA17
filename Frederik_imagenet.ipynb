{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras & Tensorflow multiclass classification\n",
    "https://www.codesofinterest.com/2017/08/bottleneck-features-multi-class-classification-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lsda/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dropout, Flatten, Dense  \n",
    "from keras import applications  \n",
    "from keras.utils.np_utils import to_categorical  \n",
    "import matplotlib.pyplot as plt  \n",
    "import math  \n",
    "import cv2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 224, 224  # image dimensions\n",
    "\n",
    "top_model_weights_path = 'models/bottleneck_fc_model.h5'  # the top layer\n",
    "train_data_dir = '../data/train/'\n",
    "validation_data_dir = '../data/valid/'\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = applications.VGG16(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 192171 images belonging to 128 classes.\n",
      "12011/12011 [==============================] - 4402s 367ms/step\n",
      "Found 6309 images belonging to 128 classes.\n",
      "395/395 [==============================] - 227s 574ms/step\n",
      "Found 192171 images belonging to 128 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1. / 255)  \n",
    "\n",
    "generator = datagen.flow_from_directory(  \n",
    " train_data_dir,  \n",
    " target_size=(img_width, img_height),  \n",
    " batch_size=batch_size,  \n",
    " class_mode=None,  \n",
    " shuffle=False)  \n",
    "\n",
    "nb_train_samples = len(generator.filenames)  \n",
    "num_classes = len(generator.class_indices)  \n",
    "\n",
    "predict_size_train = int(math.ceil(nb_train_samples / batch_size))  \n",
    "\n",
    "bottleneck_features_train = model.predict_generator(  \n",
    " generator, predict_size_train, verbose=1)  \n",
    "\n",
    "#np.save('bottleneck_features_train.npy', bottleneck_features_train)  \n",
    "\n",
    "generator = datagen.flow_from_directory(  \n",
    " validation_data_dir,  \n",
    " target_size=(img_width, img_height),  \n",
    " batch_size=batch_size,  \n",
    " class_mode=None,  \n",
    " shuffle=False)  \n",
    "\n",
    "nb_validation_samples = len(generator.filenames)  \n",
    "\n",
    "predict_size_validation = int(math.ceil(nb_validation_samples / batch_size))  \n",
    "\n",
    "bottleneck_features_validation = model.predict_generator(  \n",
    " generator, predict_size_validation, verbose=1)  \n",
    "\n",
    "#np.save('bottleneck_features_validation.npy', bottleneck_features_validation)  \n",
    "\n",
    "# labels for training data\n",
    "datagen_top = ImageDataGenerator(rescale=1./255)  \n",
    "generator_top = datagen_top.flow_from_directory(  \n",
    "     train_data_dir,  \n",
    "     target_size=(img_width, img_height),  \n",
    "     batch_size=batch_size,  \n",
    "     class_mode='categorical',  \n",
    "     shuffle=False)  \n",
    "\n",
    "nb_train_samples = len(generator_top.filenames)  \n",
    "num_classes = len(generator_top.class_indices)  \n",
    "\n",
    "# load the bottleneck features saved earlier  \n",
    "train_data = np.load('bottleneck_features_train.npy')  \n",
    "\n",
    "# get the class lebels for the training data, in the original order  \n",
    "train_labels = generator_top.classes  \n",
    "\n",
    "# convert the training labels to categorical vectors  \n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)  \n",
    "\n",
    "# labels for validation features\n",
    "generator_top = datagen_top.flow_from_directory(  \n",
    "     validation_data_dir,  \n",
    "     target_size=(img_width, img_height),  \n",
    "     batch_size=batch_size,  \n",
    "     class_mode=None,  \n",
    "     shuffle=False)  \n",
    "\n",
    "nb_validation_samples = len(generator_top.filenames)  \n",
    "\n",
    "validation_data = np.load('bottleneck_features_validation.npy')  \n",
    "\n",
    "validation_labels = generator_top.classes  \n",
    "validation_labels = to_categorical(validation_labels, num_classes=num_classes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and train top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 192171 images belonging to 128 classes.\n",
      "Found 6309 images belonging to 128 classes.\n"
     ]
    }
   ],
   "source": [
    "# RELOADING\n",
    "# labels for training data\n",
    "datagen_top = ImageDataGenerator(rescale=1./255)  \n",
    "generator_top = datagen_top.flow_from_directory(  \n",
    "     train_data_dir,  \n",
    "     target_size=(img_width, img_height),  \n",
    "     batch_size=batch_size,  \n",
    "     class_mode='categorical',  \n",
    "     shuffle=False)  \n",
    "\n",
    "nb_train_samples = len(generator_top.filenames)  \n",
    "num_classes = len(generator_top.class_indices)  \n",
    "\n",
    "# load the bottleneck features saved earlier  \n",
    "train_data = np.load('bottleneck_features_train.npy')  \n",
    "\n",
    "# get the class lebels for the training data, in the original order  \n",
    "train_labels = generator_top.classes  \n",
    "\n",
    "# convert the training labels to categorical vectors  \n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes) \n",
    "\n",
    "# labels for validation features\n",
    "generator_top = datagen_top.flow_from_directory(  \n",
    "     validation_data_dir,  \n",
    "     target_size=(img_width, img_height),  \n",
    "     batch_size=batch_size,  \n",
    "     class_mode=None,  \n",
    "     shuffle=False)  \n",
    "\n",
    "nb_validation_samples = len(generator_top.filenames)  \n",
    "\n",
    "validation_data = np.load('bottleneck_features_validation.npy')  \n",
    "validation_labels = generator_top.classes  \n",
    "validation_labels = to_categorical(validation_labels, num_classes=num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 192171 samples, validate on 6309 samples\n",
      "Epoch 1/50\n",
      "192171/192171 [==============================] - 129s 672us/step - loss: 1.5084 - acc: 0.0376 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 2/50\n",
      "192171/192171 [==============================] - 113s 588us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 3/50\n",
      "192171/192171 [==============================] - 113s 587us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 4/50\n",
      "192171/192171 [==============================] - 113s 587us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 5/50\n",
      "192171/192171 [==============================] - 113s 588us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 6/50\n",
      "192171/192171 [==============================] - 113s 588us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 7/50\n",
      "192171/192171 [==============================] - 113s 589us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 8/50\n",
      "192171/192171 [==============================] - 113s 587us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 9/50\n",
      "192171/192171 [==============================] - 113s 588us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 10/50\n",
      "192171/192171 [==============================] - 113s 586us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 11/50\n",
      "192171/192171 [==============================] - 113s 590us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 12/50\n",
      "192171/192171 [==============================] - 113s 588us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 13/50\n",
      "192171/192171 [==============================] - 113s 586us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 14/50\n",
      "192171/192171 [==============================] - 113s 586us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 15/50\n",
      "192171/192171 [==============================] - 112s 585us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 16/50\n",
      "192171/192171 [==============================] - 113s 587us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 17/50\n",
      "192171/192171 [==============================] - 113s 587us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 18/50\n",
      "192171/192171 [==============================] - 113s 589us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 19/50\n",
      "192171/192171 [==============================] - 113s 586us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 20/50\n",
      "192171/192171 [==============================] - 113s 587us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 21/50\n",
      "192171/192171 [==============================] - 113s 590us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 22/50\n",
      "192171/192171 [==============================] - 112s 585us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 23/50\n",
      "192171/192171 [==============================] - 113s 586us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 24/50\n",
      "192171/192171 [==============================] - 113s 588us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 25/50\n",
      "192171/192171 [==============================] - 113s 587us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 26/50\n",
      "192171/192171 [==============================] - 113s 587us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 27/50\n",
      "192171/192171 [==============================] - 113s 588us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 28/50\n",
      "192171/192171 [==============================] - 113s 586us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 29/50\n",
      "192171/192171 [==============================] - 113s 586us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 30/50\n",
      "192171/192171 [==============================] - 113s 587us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 31/50\n",
      "192171/192171 [==============================] - 112s 584us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 32/50\n",
      "192171/192171 [==============================] - 112s 585us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 33/50\n",
      "192171/192171 [==============================] - 112s 585us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 34/50\n",
      "192171/192171 [==============================] - 113s 587us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 35/50\n",
      "192171/192171 [==============================] - 113s 587us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 36/50\n",
      "192171/192171 [==============================] - 113s 585us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 37/50\n",
      "192171/192171 [==============================] - 112s 584us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 38/50\n",
      "192171/192171 [==============================] - 112s 585us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 39/50\n",
      "192171/192171 [==============================] - 112s 584us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 40/50\n",
      "192171/192171 [==============================] - 113s 587us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 41/50\n",
      "192171/192171 [==============================] - 113s 586us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 42/50\n",
      "192171/192171 [==============================] - 113s 585us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 43/50\n",
      "192171/192171 [==============================] - 112s 585us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 44/50\n",
      "192171/192171 [==============================] - 113s 586us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 45/50\n",
      "192171/192171 [==============================] - 113s 586us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 46/50\n",
      "192171/192171 [==============================] - 113s 588us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 47/50\n",
      "192171/192171 [==============================] - 113s 587us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 48/50\n",
      "192171/192171 [==============================] - 112s 585us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 49/50\n",
      "192171/192171 [==============================] - 113s 587us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "Epoch 50/50\n",
      "192171/192171 [==============================] - 113s 586us/step - loss: 1.1921e-07 - acc: 0.0064 - val_loss: 1.1921e-07 - val_acc: 0.0079\n",
      "6309/6309 [==============================] - 1s 153us/step\n",
      "[INFO] accuracy: 0.79%\n",
      "[INFO] Loss: 1.1920930376163597e-07\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()  \n",
    "model.add(Flatten(input_shape=train_data.shape[1:]))  \n",
    "model.add(Dense(256, activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(num_classes, activation='sigmoid'))  \n",
    "\n",
    "model.compile(optimizer='rmsprop',  \n",
    "          loss='categorical_crossentropy', metrics=['accuracy'])  \n",
    "\n",
    "history = model.fit(train_data, train_labels,  \n",
    "      epochs=epochs,  \n",
    "      batch_size=batch_size,  \n",
    "      validation_data=(validation_data, validation_labels))  \n",
    "\n",
    "model.save_weights(top_model_weights_path)  \n",
    "\n",
    "(eval_loss, eval_accuracy) = model.evaluate(  \n",
    " validation_data, validation_labels, batch_size=batch_size, verbose=1)\n",
    "\n",
    "print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100))  \n",
    "print(\"[INFO] Loss: {}\".format(eval_loss)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)  \n",
    "\n",
    "# summarize history for accuracy  \n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['acc'])  \n",
    "plt.plot(history.history['val_acc'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "\n",
    "# summarize history for loss  \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "def get_prediction(image_path):\n",
    "    #print(\"[INFO] loading and preprocessing image...\")  \n",
    "    image = load_img(image_path, target_size=(224, 224))  \n",
    "    image = img_to_array(image)  \n",
    "\n",
    "    # important! otherwise the predictions will be '0'  \n",
    "    image = image / 255  \n",
    "\n",
    "    image = np.expand_dims(image, axis=0)  \n",
    "    # run image through pipeline\n",
    "    # build the VGG16 network  \n",
    "    #model = applications.VGG16(include_top=False, weights='imagenet')  \n",
    "\n",
    "    # get the bottleneck prediction from the pre-trained VGG16 model  \n",
    "    #bottleneck_prediction = model.predict(image)  \n",
    "    bottleneck_prediction = vgg16_model.predict(image)\n",
    "\n",
    "    # build top model  \n",
    "    model = Sequential()  \n",
    "    model.add(Flatten(input_shape=bottleneck_prediction.shape[1:]))  \n",
    "    model.add(Dense(256, activation='relu'))  \n",
    "    model.add(Dropout(0.5))  \n",
    "    model.add(Dense(num_classes, activation='sigmoid'))  \n",
    "\n",
    "    model.load_weights(top_model_weights_path)  \n",
    "\n",
    "    # use the bottleneck prediction on the top model to get the final classification  \n",
    "    class_predicted = model.predict_classes(bottleneck_prediction) \n",
    "\n",
    "    inID = class_predicted[0]  \n",
    "\n",
    "    class_dictionary = generator_top.class_indices  \n",
    "\n",
    "    inv_map = {v: k for k, v in class_dictionary.items()}  \n",
    "\n",
    "    label = inv_map[inID]  \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f297658bbd479e9e1508a1549e6e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12800), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/test/320.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f208c8d2d7f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mpredicted_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-064c2d4186f7>\u001b[0m in \u001b[0;36mget_prediction\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#print(\"[INFO] loading and preprocessing image...\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, target_size, interpolation)\u001b[0m\n\u001b[1;32m    385\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    386\u001b[0m                           'The use of `array_to_img` requires PIL.')\n\u001b[0;32m--> 387\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2547\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2548\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2549\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/test/320.jpg'"
     ]
    }
   ],
   "source": [
    "# Kaggle submission\n",
    "import json\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "\n",
    "json_data = json.load(open('kaggle/test.json'))\n",
    "test_dir = '../data/test/'\n",
    "\n",
    "ids = []\n",
    "predicted_labels = []\n",
    "\n",
    "for i in tqdm_notebook(json_data['images']):\n",
    "    pred = get_prediction(test_dir + str(i['image_id']) + '.jpg')\n",
    "    ids.append(i['image_id'])\n",
    "    predicted_labels.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "my_submission = pd.DataFrame({'id': ids, 'predicted': predicted_labels})\n",
    "file_name = 'submission_' + str(datetime.datetime.now()) +'.csv'\n",
    "my_submission.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
